<!DOCTYPE html>
<html lang="zh-Hans">

<!-- Head tag -->
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="ä»Šå¤©ç»“æŸäº†Andrew Ngçš„Deep Learningç³»åˆ—è¯¾ç¨‹Class1:Neural Networks and Deep Learningã€‚ä¹ é¢˜ä¸­å…¬å¼éƒ¨åˆ†Ngä»…ç»™äº†ç»“è®ºï¼Œæ­¤å¤„å°†ç»™å‡ºå®Œæ•´æ¨å¯¼ã€‚æ­¤å¤–ï¼Œåœ¨Week3ç»ƒä¹ ä¸­æˆ‘å¯¹Logistic Regressionå’ŒNeural Networkä¹‹é—´çš„è”ç³»å’ŒåŒºåˆ«æ¯”è¾ƒæ¨¡ç³Šï¼Œæœ¬æ–‡ä¹Ÿä¼šå°†äºŒè€…è”ç³»èµ·æ¥ç¨ä½œæ€»ç»“ã€‚">
    

    <!--Author-->
    
        <meta name="author" content="ringo">
    

    <!-- Title -->
    
    <title>ä»LR åˆ° NN | æ—æˆˆçš„å°é…’é¦†ã€‚</title>

    <!-- Bootstrap Core CSS -->
    <link href="//cdn.bootcss.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Custom Fonts -->
    <link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body  background-color:rgb(255,241,229);>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Content -->
    <section class="article-container">
<!-- Back Home -->
<a class="nav-back" href="/">
    <i class="fa fa-puzzle-piece"></i>
</a>

<!-- Page Header -->
<header class="intro-header">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>ä»LR åˆ° NN</h1>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <!-- Post Main Content -->
            <div class="post-content col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <p>Â Â Â Â Â Â Â Â ä»Šå¤©ç»“æŸäº†Andrew Ngçš„Deep Learningç³»åˆ—è¯¾ç¨‹Class1:<em>Neural Networks and Deep Learning</em>ã€‚ä¹ é¢˜ä¸­å…¬å¼éƒ¨åˆ†Ngä»…ç»™äº†ç»“è®ºï¼Œæ­¤å¤„å°†ç»™å‡ºå®Œæ•´æ¨å¯¼ã€‚æ­¤å¤–ï¼Œåœ¨Week3ç»ƒä¹ ä¸­æˆ‘å¯¹Logistic Regressionå’ŒNeural Networkä¹‹é—´çš„è”ç³»å’ŒåŒºåˆ«æ¯”è¾ƒæ¨¡ç³Šï¼Œæœ¬æ–‡ä¹Ÿä¼šå°†äºŒè€…è”ç³»èµ·æ¥ç¨ä½œæ€»ç»“ã€‚</p>
<h1>1.ä»Linear Regressionè¯´èµ·</h1>
<p>Â Â Â Â Â Â Â Â åœ¨ä»‹ç»é€»è¾‘å›å½’(Logistic Regression)ä¹‹å‰ï¼Œæˆ‘æƒ³å›é¡¾ä¸€ä¸‹çº¿æ€§å›å½’(Linear Regression)æ–¹æ³•ã€‚çº¿æ€§å›å½’ä¸ä»…èƒ½è§£å†³çº¿æ€§åˆ†ç±»é—®é¢˜ï¼Œå¤šå…ƒçº¿æ€§å›å½’ä¹Ÿèƒ½è§£å†³éçº¿æ€§åˆ†ç±»é—®é¢˜ã€‚å®ƒäº§ç”Ÿçš„é¢„æµ‹æ˜¯è¿ç»­çš„ï¼Œ<strong>è¿™ä¸ªç‰¹æ€§ä½¿å®ƒé€‚åˆå¯¹è¿ç»­å˜é‡è¿›è¡Œé¢„æµ‹</strong>ã€‚é’ˆå¯¹ä¸€ç»„è¿ç»­æˆ–ç¦»æ•£çš„åºåˆ—å¯¹{$(x_0, y_0), (x_1, y_1), ...(x_n, y_n)$}, ä¾‹å¦‚å›¾åƒå¦‚ä¸‹ï¼š</p>
<p><img src="http://www.stat.yale.edu/Courses/1997-98/101/multregr.gif" alt="Linear Classification"></p>
<p>Â Â Â Â Â Â Â Â å‡è®¾å…¶featureä¸ªæ•°ä¸º$m$(æŸæ ·æœ¬$x$ä¸­å°±åŒ…å«å¾ˆå¤šç»´åº¦çš„æ•°æ®)ï¼Œ å³ä½¿ç”¨$m$å…ƒçº¿æ€§å‚æ•°æ‹Ÿåˆè¯¥æ•°æ®ï¼Œåˆ™é’ˆå¯¹æŸä¸€ä¸ªæ ·æœ¬$(x_i, y_i)$ï¼Œå‡è®¾å‡½æ•°(hypothesis function)$h_\theta(x_i)$ä¸ºï¼š</p>
<p>$$h_\theta(x_i) = \sum_{j=1}^{m}\theta_j {x^{[j]}_i}$$</p>
<p>Â Â Â Â Â Â Â Â æ¥ç€æˆ‘ä»¬ä½¿ç”¨å¹³æ–¹æŸå¤±å‡½æ•°(loss function)è¿›è¡Œæ±‚é¢„æµ‹å’Œå®é™…çš„å¹³æ–¹å·®ï¼š</p>
<p>$$L(\theta) = \frac{1}{2m}\sum_{j=1}^{m}(h_{\theta}(x^i) - y^i)^2$$</p>
<p>Â Â Â Â Â Â Â Â å†ä½¿ç”¨gradient descent, å¯¹è¿™$m$å…ƒå‚æ•°$\theta$:</p>
<p>$$\theta := \theta - \alpha \cdot \frac{1}{m} \cdot x^T \cdot (x \cdot \theta - y)$$</p>
<p>å¹³æ–¹æŸå¤±å‡½æ•°æ˜¯å‡¸å‡½æ•°ï¼Œ å› æ­¤åç»­å·¥ä½œå°±æ˜¯ä¸æ–­è¿­ä»£ç›´è‡³æ”¶æ•›ï¼Œå…¶ä¸­è¦æ³¨æ„2ä¸ªé—®é¢˜ï¼š</p>
<ul>
<li>è€ƒè™‘åˆ°$m$è¶Šå¤§è®­ç»ƒè¯¯å·®è¶Šå°ï¼Œæœ€ç»ˆå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå› æ­¤è¦åŠ æ­£åˆ™é¡¹ã€‚</li>
<li>æå¤§ä¼¼ç„¶ä¼°è®¡(maximum likelihood estimation)ç­‰ä»·äºæœ€å°åŒ–å¹³æ–¹æŸå¤±(minimization of MSE)çš„å‰ææ˜¯ï¼šæ ·æœ¬å™ªå£°ç¬¦åˆé«˜æ–¯åˆ†å¸ƒã€‚è‹¥æ•°æ®å™ªå£°ä¸ç¬¦åˆé«˜æ–¯åˆ†å¸ƒï¼Œä»è¦æŒ‰æå¤§ä¼¼ç„¶ä¼°è®¡æ€æƒ³è¿›è¡ŒæŸå¤±è®¡ç®—ï¼Œå¯èƒ½è¦æ”¹ä¸ºè®¡ç®—äº¤å‰ç†µ(cross-entropy)ã€‚</li>
</ul>
<h1>2. Logistic Regression</h1>
<p>Â Â Â Â Â Â Â Â æ­¤å¤„æ¬è¿Andrew Ngè¯¾ç¨‹<a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="external">Neural Networks and Deep Learning</a>ä½œä¸š(Week2, Assignment2)ä¸­çš„å®ä¾‹ï¼š
æˆ‘ä»¬å°†ä¸€å¼ $64 \times 64$çš„å›¾åƒåˆ†ä¸ºRGBä¸‰å±‚ï¼Œå°†è¿™$64 \times 64 \times 3$å…±12288ä¸ªåƒç´ (å…¶å€¼åœ¨$[0, 255]$ä¹‹é—´)æ•´åˆä¸º1åˆ—ï¼Œå› æ­¤æŒ‰ç…§Linear Regressionä¸­çš„è¯´æ³•ï¼Œ$x$çš„â€œå‚æ•°æœ‰12288å…ƒâ€ï¼Œå¦‚ä¸‹å›¾ï¼š</p>
<p><img src="ng-lr.png" alt=""></p>
<p>Â Â Â Â Â Â Â Â è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š</p>
<p>$$z^{(i)} = w^T x^{(i)} + b \tag{1}$$</p>
<p>$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\tag{2}$$</p>
<p>$$ \mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \log(a^{(i)}) - (1-y^{(i)} )  \log(1-a^{(i)})\tag{3}$$</p>
<p>$$ J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{4}$$</p>
<p>Â Â Â Â Â Â Â Â ç”¨ä¸€å¼ <a href="http://ronny.rest/blog/post_2017_08_12_logistic_regression_derivative/" target="_blank" rel="external">å›¾</a>æè¿°æ­¤è¿‡ç¨‹ï¼š</p>
<p><img src="http://ronny.rest/media/blog/2017/2017_08_12_logistic_regression_derivative/logistic_regression_graph.png" alt="lr"></p>
<p>é€»è¾‘å›å½’ç›¸æ¯”çº¿æ€§å›å½’ï¼Œå·®å¼‚ä»å…¬å¼(2)(3)ä¸­å·²åˆè§ç«¯å€ªï¼š</p>
<p>Â Â Â Â Â Â Â Â <strong>1. æŸå¤±å‡½æ•°çš„è®¡ç®—ä»MSEå˜ä¸ºcross-entropyã€‚</strong></p>
<p>Â Â Â Â Â Â Â Â ç¥ç»ç½‘ç»œä¸­&quot;The choice of cost function is tightly coupled with the choice of output unit&quot;çš„è¯´æ³•åœ¨logistic regressionä¸­åŒæ ·é€‚ç”¨ã€‚sigmoidå‡½æ•°çš„ç‰¹æ€§å†³å®šäº†å®ƒåœ¨$y \approx 1$å’Œ$y \approx -1$æ—¶å‡ºç°â€œæ¢¯åº¦æ¶ˆå¤±â€(saturation)ã€‚å› æ­¤$log$å½¢å¼çš„cross-entropyèƒ½æå¤§åœ°å‡ç¼“æ­¤æ•ˆåº”ï¼Œåªæœ‰åœ¨$y \approx 1 $ä¸”$z$æ˜¯å¾ˆå¤§çš„æ­£å€¼ï¼Œæˆ–è€…$y \approx -1 $ä¸”$z$æ˜¯å¾ˆå°çš„è´Ÿå€¼æ—¶ï¼Œæ­¤ç°è±¡æ‰ä¼šå‘ç”Ÿã€‚<em>Ian Goodfellow</em>çš„<em>Deep Learning</em>ä¸­ç»™å‡ºäº†è¯¦ç»†æ¨å¯¼è¿‡ç¨‹ã€‚</p>
<p>Â Â Â Â Â Â Â Â <strong>2. å¢åŠ äº†æ¿€æ´»å‡½æ•°(activation function)ã€‚</strong></p>
<p>Â Â Â Â Â Â Â Â è¿™æ˜¯å¾ˆå…³é”®çš„ä¸€æ­¥ï¼Œå®ƒç›¸å½“äºå¯¹åŸæœ‰çš„çº¿æ€§åŠ æƒå‡½æ•°åšäº†ä¸€ä¸ªéçº¿æ€§å˜æ¢ã€‚é‚£ä¹ˆéçº¿æ€§å˜æ¢çš„æ„ä¹‰åœ¨å“ªï¼Ÿ</p>
<p>Â Â Â Â Â Â Â Â ä¸ºäº†æ›´æ¸…æ™°åœ°è§£é‡Šéçº¿æ€§å˜æ¢çš„æ„ä¹‰ï¼Œæ­¤å¤„å°†æ¿€æ´»å‡½æ•°ç®€åŒ–ä¸ºReLU(Rectified Linear Unit) function, å…¶å‡½æ•°å½¢å¼ï¼š</p>
<p>$$f(x)=max(0, x)$$</p>
<p>Â Â Â Â Â Â Â Â å›¾åƒå¦‚ä¸‹ï¼š</p>
<p><img src="https://relinklabs.com/imager/uploads/images/1107/Screen-Shot-2017-06-23-at-1.27.11-PM_d4e4e04a52bfbdbeab5ba4d2cf1d1390.png" alt="ReLU"></p>
<p>Â Â Â Â Â Â Â Â å›¾ä¸­æ©˜é»„è‰²çš„ç›´çº¿æ˜¯å¯¹ReLUçš„æ”¹è¿›ï¼Œç§°ä¸ºleaky ReLUã€‚</p>
<p>Â Â Â Â Â Â Â Â å‡è®¾ç°åœ¨æˆ‘ä»¬è¦ä½¿ç”¨çº¿æ€§æ¨¡å‹æ„é€ å¼‚æˆ–é—¨(XOR)ï¼Œå³å¯¹å¦‚ä¸‹å‡ ç»„è¾“å…¥ï¼š
$$x_1 = [0, 0] \qquad x_2 = [0, 1] \qquad x_3=[1, 0] \qquad x_4=[1, 1] $$</p>
<p>Â Â Â Â Â Â Â Â å¸Œæœ›$y$å€¼åˆ†åˆ«æ˜¯:
$$[0, 1, 1, 0]$$</p>
<p>Â Â Â Â Â Â Â Â æŒ‰ç…§çº¿æ€§å‡½æ•°çš„å¹³æ–¹æŸå¤±ï¼çº¿æ€§æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå…¶æœ€ç»ˆå‚æ•°$w$ä¸º0ï¼Œ$b$ä¸º0.5ï¼Œå³å¯¹ä»»ä½•è¾“å…¥$x$ï¼Œå…¶é¢„æµ‹ç»“æœå‡ä¸º$\frac{1}{2}$ï¼Œè¿™æ˜¾ç„¶ä¸ç¬¦åˆé¢„æœŸã€‚<strong>çº¿æ€§æ¨¡å‹çš„ä¸€å¤§é—®é¢˜åœ¨äºå®ƒåªå¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œçº¿æ€§åŠ æƒï¼Œæ— æ³•å­¦ä¹ â€œä¸¤ä¸ªç‰¹å¾ä¹‹é—´çš„äº¤äº’ä½œç”¨â€</strong>ã€‚æˆ‘ä»¬å°†è¾“å…¥æ•°æ®å±•ç¤ºå‡ºæ¥ï¼š
<img src="https://sfault-image.b0.upaiyun.com/159/297/1592977061-59c27f1d46a1a_articlex" alt=""></p>
<p>çº¿æ€§å›å½’å›°å¢ƒåœ¨äºï¼š</p>
<ul>
<li>å½“$x_1=0$æ—¶ï¼Œ$y$éšç€$x_2$çš„å¢å¤§è€Œå¢å¤§(ä»0-&gt;1)</li>
<li>å½“$x_1=1$æ—¶ï¼Œ$y$éšç€$x_2$çš„å¢å¤§è€Œå‡å°(ä»1-&gt;0)</li>
</ul>
<p>Â Â Â Â Â Â Â Â åœ¨è¿™é‡Œï¼ŒLogistic Regressionå¢åŠ äº†æ¿€æ´»å‡½æ•°ï¼Œç›¸å½“äºå¯¹åŸçº¿æ€§å‡½æ•°åšäº†ä¸€ä¸ªéçº¿æ€§å˜æ¢ï¼Œäºæ˜¯å‡è®¾å‡½æ•°å°†å˜ä¸ºï¼š</p>
<p>$$f(x, W, c, \omega, b) = {\omega}^Tmax\{0, W^Tx + c\} + b \tag{5}$$</p>
<p>Â Â Â Â Â Â Â Â å…¶å®å°±æ˜¯åœ¨å…¬å¼(1)å¤–å¥—äº†ä¸€ä¸ªéçº¿æ€§å‡½æ•°ã€‚æ­¤æ—¶è®¾ç½®ï¼š</p>
<p>$$W = \begin {bmatrix}1 &amp; 1\\ 1 &amp; 1\end{bmatrix}$$</p>
<p>$$c = \begin {bmatrix}0 \\ -1 \end{bmatrix}$$</p>
<p>$$\omega = \begin {bmatrix}1 \\ -2 \end{bmatrix}$$</p>
<p>$$b=0$$</p>
<p>Â Â Â Â Â Â Â Â æŒ‰æ¡ä»¶ï¼š</p>
<p>$$X = \begin {bmatrix}0 &amp; 0\\ 0 &amp; 1\\ 1 &amp; 0\\ 1 &amp; 1\end{bmatrix}$$</p>
<p>Â Â Â Â Â Â Â Â äºæ˜¯ï¼š</p>
<p>$$XW = \begin {bmatrix}0 &amp; 0\\ 1 &amp; 1\\ 1 &amp; 1\\ 2 &amp; 2\end{bmatrix}$$</p>
<p>Â Â Â Â Â Â Â Â å†åŠ ä¸Š$c$ï¼š</p>
<p>$$XW+c = \begin {bmatrix}0 &amp; -1\\ 1 &amp; 0\\ 1 &amp; 0\\ 2 &amp; 1\end{bmatrix}$$</p>
<p>Â Â Â Â Â Â Â Â æ ¹ç»ReLUç‰¹æ€§ï¼š</p>
<p>$$max\{0, XW+c\} = \begin {bmatrix}0 &amp; 0\\ 1 &amp; 0\\ 1 &amp; 0\\ 2 &amp; 1\end{bmatrix}$$</p>
<p>Â Â Â Â Â Â Â Â å…¶å›¾åƒä¸ºï¼š</p>
<p><img src="https://sfault-image.b0.upaiyun.com/249/631/2496315672-59c27ff9e9e58_articlex" alt=""></p>
<p>Â Â Â Â Â Â Â Â æˆ‘ä»¬å‘ç°æ­¤æ—¶å·²ç»çº¿æ€§å¯åˆ†äº†ï¼Œå¤–å±‚ä½¿ç”¨çº¿æ€§å‡½æ•°å³å¯åˆ’åˆ†å‡ºè¶…å¹³é¢ï¼š</p>
<p>$${\omega}^Tmax\{0, XW + c\} + b  = \begin {bmatrix}0 \\ 1 \\ 1 \\ 0\end{bmatrix}$$</p>
<p>Â Â Â Â Â Â Â Â ä¸€ä¸ªä¸è§„èŒƒçš„è¡¨è¿°æ˜¯ï¼šéçº¿æ€§çš„æ¿€æ´»å‡½æ•°èƒ½&quot;å­¦ä¹ &quot;ç‰¹å¾ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œä»è€Œä½¿çº¿æ€§å›å½’ä¸å¯åˆ†(åŒºåˆ«äºçº¿æ€§ä¸å¯åˆ†)çš„æ•°æ®å˜çš„å¯åˆ†ã€‚é¡ºä¾¿ä¸€æï¼ŒReLUå‡½æ•°ç›¸æ¯”sigmoidå‡½æ•°ï¼Œæœ‰å¦‚ä¸‹ä¼˜åŠ¿ï¼š</p>
<ul>
<li>ReLUçš„è¿‘ä¼¼çº¿æ€§ï¼Œä½¿å…¶åœ¨åå‘æ¢¯åº¦è®¡ç®—æ—¶æ–¹ä¾¿è®¡ç®—å’Œæ¨æ¼”ã€‚è€Œäº‹å®ä¸Šå®ƒæ˜¯éçº¿æ€§çš„ï¼Œå› æ­¤èƒ½è§£å†³çº¿æ€§å›å½’ä¸èƒ½è§£å†³çš„åˆ†ç±»å›°å¢ƒã€‚</li>
<li>åå‘ä¼ æ’­æ¢¯åº¦ä¸‹é™æ›´å¿«ã€‚sigmoidå‡½æ•°è¢«å¼ƒç”¨çš„ä¸€ä¸ªé‡è¦åŸå› å°±æ˜¯å…¶åœ¨$[-1,1]$åŒºé—´æ¥è¿‘ä¸¤ç«¯çš„ä½ç½®æ¢¯åœ°ä¸‹é™ç¼“æ…¢ï¼Œè®­ç»ƒæ—¶é—´å¾ˆé•¿ã€‚</li>
</ul>
<h1>3. LRçš„ä¸€äº›æ¨å¯¼å’Œæ€è€ƒ</h1>
<p>Â Â Â Â Â Â Â Â ä¸Šæ–‡å…¬å¼(4)ä¸­ï¼Œæˆ‘ä»¬å·²ç»ç»™å‡ºäº†æŸå¤±å‡½æ•°$J$çš„è®¡ç®—æ–¹æ³•ï¼Œå‰©ä¸‹çš„å°±æ˜¯æ±‚æ¢¯åº¦ã€‚Ngåœ¨Programming Assignmentä¸­ç»™å‡ºäº†å¦‚ä¸‹å…¬å¼ï¼š</p>
<p>$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{6}$$
$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{7}$$</p>
<p>Â Â Â Â Â Â Â Â ä¸‹é¢ç»™å‡ºæ¨å¯¼è¿‡ç¨‹ã€‚å¯¹æŸä¸ªæ ·æœ¬ï¼Œå…¶é¢„æµ‹å€¼ä¸º$a$ï¼ŒçœŸå®å€¼ä¸º$y$, å‚æ•°åˆ†åˆ«ä¸º$\omega$å’Œ$b$ï¼Œåˆ™å½“å‰å·²çŸ¥æ¡ä»¶ä¸º:</p>
<p>$$\begin{cases}J=-(ylog(a)+(1-y)log(1-a)) \\ a=sigmoid(z) \\ z=\omega x + b \end{cases}$$</p>
<p>Â Â Â Â Â Â Â Â sigmoidå‡½æ•°ç‰¹æ€§å¦‚ä¸‹ï¼š</p>
<p>$$g(z) = \frac{1}{1+e^{-z}}$$</p>
<p>$$g'(z) = g(z) \cdot (1-g(z))$$</p>
<p>Â Â Â Â Â Â Â Â å› æ­¤ï¼š
$$\frac{\partial{J}}{\partial{\omega}} = \frac{\partial{J}}{\partial{a}} \cdot \frac{\partial{a}}{\partial{z}} \cdot \frac{\partial{z}}{\partial{\omega}} $$</p>
<p>$$= (-\frac{y}{a} + \frac{1-y}{1-a}) \cdot a(1-a) \cdot x$$</p>
<p>$$= (a-y)x$$</p>
<p>Â Â Â Â Â Â Â Â æ­¤æ—¶ä¸éš¾çœ‹å‡º:
$$\frac{\partial{J}}{\partial{b}} = a-y$$</p>
<p>Â Â Â Â Â Â Â Â æ•´ä¸ªLogistic Regressionåˆ°æ­¤ç»“æŸã€‚</p>
<h1>4. Neural Network</h1>
<p>Â Â Â Â Â Â Â Â å¯¹ä¸€ä¸ªNå±‚ç¥ç»ç½‘ç»œï¼Œå¦‚ä¸‹å›¾ï¼š</p>
<p><img src="dnn.png" alt=""></p>
<p>Â Â Â Â Â Â Â Â æ¯å±‚å†…éƒ¨çš„å‰å‘è®¡ç®—å…¬å¼å‡ä¸ºï¼š</p>
<p>$$\begin{cases}Z^{[l]}=W^{[l]}a^{[l-1]} + b^{[l]} \\ a^{[l]} = g(z^{[l]}) \end{cases}$$</p>
<p>Â Â Â Â Â Â Â Â å…¶ä¸­æ‰€æœ‰éšå«å±‚çš„æ¿€æ´»å‡½æ•°å‡ä¸ºRectified functionï¼Œè¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°ä¸ºsigmoid functionã€‚ç»“æ„å¦‚ä¸‹ï¼š</p>
<p><img src="sequence.png" alt=""></p>
<p>Â Â Â Â Â Â Â Â å…³äºDNNæ¢¯åº¦çš„è®¡ç®—ï¼ŒNgä¹Ÿåªç»™å‡ºäº†å…¬å¼ï¼š</p>
<p>$$ dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T} \tag{8}$$
$$ db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l]}\tag{9}$$
$$ dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \tag{10}$$</p>
<p>Â Â Â Â Â Â Â Â å…¶ä¸­ï¼š</p>
<p>$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \tag{11}$$</p>
<p>Â Â Â Â Â Â Â Â æ¨å¯¼ä¹Ÿä¸éš¾ï¼š</p>
<p>$$ dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}} \cdot \frac{\partial  Z^{[l]} }{\partial W^{[l]}}$$</p>
<p>$$= dZ^{[l]} A^{[l-1] T}$$</p>
<p>Â Â Â Â Â Â Â Â æˆ‘ä»¬å‘ç°ï¼Œè¦è®¡ç®—$dZ^{[l]}$åˆéœ€è¦$dA^{[l]} $ï¼Œè€Œæƒ³è¦çŸ¥é“$ dA^{[l]}$åˆå¾—çŸ¥é“$ dZ^{[l+1]} $ï¼Œè¿™ç§â€œé¡ºè—¤æ‘¸ç“œâ€çš„å¯»æ‰¾åªæœ‰åœ¨è¾“å‡ºå±‚æ‰ä¼šç»ˆæ­¢ï¼Œåœ¨é‚£é‡Œ:
$$da = - \frac{y}{a} + \frac{1-y}{1-a} \tag{ä¸Šæ–‡å·²æ¨å¯¼è¿‡}$$</p>
<p>Â Â Â Â Â Â Â Â å› æ­¤å¯¹ç¥ç»ç½‘ç»œæ¢¯åº¦çš„æ›´æ–°å°†ç”±æ·±å‘æµ…å±‚é€†åºè¿›è¡Œï¼Œéƒ¨åˆ†ä»£ç å¦‚ä¸‹ï¼š</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># GRADED FUNCTION: L_model_backward</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></div><div class="line">    </div><div class="line">    <span class="comment"># Initializing the backpropagation</span></div><div class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</div><div class="line">    </div><div class="line">    current_cache = caches[L<span class="number">-1</span>]</div><div class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL, current_cache, <span class="string">"sigmoid"</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># update parameters in reverse order.</span></div><div class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</div><div class="line">        current_cache = caches[l]</div><div class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">"dA"</span> + str(l+<span class="number">2</span>)], current_cache, <span class="string">"relu"</span>)</div><div class="line">        grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)] = dA_prev_temp</div><div class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</div><div class="line"></div><div class="line">    <span class="keyword">return</span> grads</div></pre></td></tr></table></figure></p>
<p>Â Â Â Â Â Â Â Â æ³¨æ„åˆ°å…¬å¼(8)(9)(10)å¹¶ä¸åƒLRä¸­ä¸€æ ·ç»™å‡ºæœ€ç»ˆæ•°æ®çš„è®¡ç®—å…¬å¼ï¼Œè€Œæ˜¯æ¯æ¬¡éƒ½ä½¿ç”¨å½“å‰çš„ä¸´æ—¶æ•°æ®ï¼Œä¾‹å¦‚è¦è®¡ç®—$ dW^{[l]} $éœ€è¦$ dZ^{[l]} $å’Œ$A^{[l-1] T}$ï¼Œå› æ­¤æ¯ä¸€å±‚éœ€è¦ç¼“å­˜å½“å‰çš„$A,W,Z,b$ç­‰ä¿¡æ¯ã€‚</p>
<p>Â Â Â Â Â Â Â Â ä¹‹åå°±æ˜¯é€‰å–ä¸€ä¸ªåˆé€‚çš„å­¦ä¹ é€Ÿç‡$\alpha$ï¼Œå¯¹$W$å’Œ$b$ä¸æ–­æ›´æ–°ã€‚æ­¤å¤„å†å›çœ‹LRï¼Œå‘ç°äºŒè€…æœ¬è´¨å¾ˆç›¸ä¼¼ï¼Œå¯ä»¥è¯´<strong>Logistic Regressionæ˜¯1-layerçš„NN</strong>ï¼Œå®ƒåªåŒ…å«è¾“å…¥å’Œè¾“å‡ºå±‚ï¼Œæ— éšå«å±‚ã€‚</p>
<h1>5. å‚è€ƒææ–™</h1>
<ul>
<li><a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="external">Andrew Ngçš„Deep Learningè¯¾ç¨‹</a></li>
<li><a href="http://ronny.rest/blog/post_2017_08_12_logistic_regression_derivative/" target="_blank" rel="external">Derivatives for logistic regression - step by step</a></li>
<li>Ian Goodfellow, Yoshua Bengio, and Aaron Courville (2017).
Deep Learning</li>
</ul>

 
                <!-- Meta -->
                <div class="post-meta">
                    <hr>
                    <br>
                    <div class="post-tags">
                        
                            

<a href="/tags/æœºå™¨å­¦ä¹ /">#æœºå™¨å­¦ä¹ </a> <a href="/tags/æ·±åº¦å­¦ä¹ /">#æ·±åº¦å­¦ä¹ </a> <a href="/tags/ç¥ç»ç½‘ç»œ/">#ç¥ç»ç½‘ç»œ</a>


                        
                    </div>
                    <div class="post-date">
                        2017 å¹´ 09 æœˆ 18 æ—¥
                    </div>
                </div>
            </div>

            <!-- Comments -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <!-- Disqus Comments -->


            </div>
        </div>
    </div>
</article>
</section>

    <!-- Scripts -->
    <!-- jQuery -->
<script src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script>
<!-- Bootstrap -->
<script src="//cdn.bootcss.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<script type="text/javascript">
	console.log('Hexo-theme-hollow designed by zchen9 ğŸ™‹ Â© 2015-' + (new Date()).getFullYear());
</script>

    <!-- Google Analytics -->
    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-98699464-1', 'auto');
        ga('send', 'pageview');

    </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>

</html>