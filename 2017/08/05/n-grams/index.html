<!DOCTYPE html>
<html lang="zh-Hans">

<!-- Head tag -->
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="从此文起，我将重心转移到对机器音乐的探索。Github项目jazzml能基于机器学习算法自动生成音乐，作者最初使用的生成模型即为N-Grams。本文是我关于Speech and Language Processing第4章N-Grams
的阅读笔记。">
    

    <!--Author-->
    
        <meta name="author" content="ringo">
    

    <!-- Title -->
    
    <title>N-Grams模型初探 | 林戈的小酒馆。</title>

    <!-- Bootstrap Core CSS -->
    <link href="//cdn.bootcss.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Custom Fonts -->
    <link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body  background-color:rgb(255,241,229);>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Content -->
    <section class="article-container">
<!-- Back Home -->
<a class="nav-back" href="/">
    <i class="fa fa-puzzle-piece"></i>
</a>

<!-- Page Header -->
<header class="intro-header">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>N-Grams模型初探</h1>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <!-- Post Main Content -->
            <div class="post-content col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <p>        从此文起，我将重心转移到对机器音乐的探索。Github项目jazzml能基于机器学习算法自动生成音乐，作者最初使用的生成模型即为N-Grams。本文是我关于<em>Speech and Language Processing</em>第4章<em>N-Grams</em>
的阅读笔记。</p>
<h1>1.问题引入</h1>
<p>        我们首先从一个有趣的问题开始：下面句子中接下来的词会是什么？</p>
<p>                <em>Please turn your homework ...</em></p>
<p>        我们直觉的判断可能会是<em>over</em>，但绝不可能是<em>the</em>或者<em>run</em>。接下来我们会把这种直观上的感觉转化成概率模型。这个模型将能使:</p>
<p>                <em>all of a sudden I notice three guys standing on the sidewalk</em></p>
<p>        与</p>
<p>                <em>on guys all I of notice sidewalk three a sudden standing the</em></p>
<p>        相比较而言，前者的整体概率更高。更直白一些的说法是：句子中语序正确程度与其整体概率成正比。</p>
<p>        我们能用这个模型做什么呢？首先，如果能准确预测接下来的词(word)是什么，现有的手机输入法提示性能将大大改善。对全文概率的预测，还可以辅助机器翻译(machine translation)中对语序进行调整，也能用于拼写纠错(spelling correction)和手写识别(handwriting recognition)。</p>
<h1>2.N-Grams</h1>
<p>        <strong>N-Grams</strong>是一个包含N个连续词的序列。特别地，我们将仅包含1个词的序列称为<em>unigram</em>，包含2个词的为<em>bigram</em>, 包含3个称为<em>trigram</em>。以语句<em>Please turn your homework</em>为例，其unigram、bigram、trigram分别为：
<figure class="highlight coq"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">unigram：Please | <span class="type">turn</span> | <span class="type">your</span> | <span class="type">homework</span></div><div class="line">bigram： Please turn | <span class="type">turn</span> your | <span class="type">your</span> homework</div><div class="line">trigram：Please turn your | <span class="type">turn</span> your homework</div></pre></td></tr></table></figure></p>
<p>        我们回忆上文回答问题时用到的词——直觉。是的，直觉告诉我们<em>turn your homework over</em>比<em>turn your homework the</em>可能性更大，这源于我们在日常会话和文字阅读过程中积累的经验。现在我们试图将这种经验使用概率模型表达：给定历史词序列$h$，计算下一个词是$w$的概率。例如：
$$P(the | its　water　is　 so 　transparent　 that)$$</p>
<p>        最简单的做法是收集一个巨大的数据集，统计词<em>the</em>出现在语句<em>its water is so transparent that</em>之后的次数：
$P(the | its　water　is　 so 　transparent　 that) = \frac{P(its　water　is　 so 　transparent　 that　the)}{P(its　water　is　 so 　transparent　 that)}$</p>
<p>        但实际上，即使数据量大到包含整个万维网，也不够给出令我们满意的结果。究其原因，是语言本身富于创造性导致对任何一条句子的微小扩展，都可能在网页上找不到完全相同的句子。例如：<em>Walden Pond's water is so transparent that the</em>在网页上就找不到这一句。另外，每当我们想评估一条新的语句，例如$P(transparent |water　is　 so 　transparent)$，就得重新统计，这无疑增加了工作量。</p>
<h2>2.1 N-Grams模型</h2>
<p>        我们首先给出计算词序列的联合概率公式：历史词序列$h$包含{$w_1, ..., w_n$}共$n$个词，用$w_i^j$表示第$i$到第$j$个词，则该词序列的联合概率:
$$P(w_1^n)=P(w_1)P(w_2|w_1)P(w_3|w_1^2)...P(w_n|w_1^{n-1})$$</p>
<p>$$=\prod_{k=1}^nP(X_k|X_1^{k-1})$$</p>
<p>        上式的假设条件是：当前词$w_i$的概率仅与它之前的词{$w_1, ..., w_{n-1}$}有关。那么我们如何计算$P(w_k|w_1^{k-1})$呢？如上文所说，此项计算依赖海量数据，并需要进行大量统计。</p>
<p>        N-Grams模型的灵感源自：与其计算某个词$w_i$在给定$history$出现的概率，我们不妨仅以该词最邻近的N个词{$w_{i-N+1}, ... w_{i-1}$}近似代替整个$history$，让我们以最简单的<em>bigram</em>举例。</p>
<h3>bigram</h3>
<p>        在<em>bigram</em>中我们仅计算词$w_i$对其前1个词$w_{i-1}$的条件概率，因此对$P(w_k|w_1^{k-1})$的计算变为$P(w_k|w_{k-1})$。即与其计算：
$$P(the | its　water　is　 so 　transparent　 that)$$</p>
<p>现在我们只需要计算：
$$P(the |that)$$</p>
<p>        因此当我们在使用<em>bigram</em>时，其实我们是在做如下近似：
$$P(w_n|w_1^{n-1}) \approx P(w_n|w_{n-1})$$</p>
<p>        这种“假设当前词的概率仅取决于前1个词”的假设称之为<a href="https://en.wikipedia.org/wiki/Markov_property" target="_blank" rel="external">马尔科夫假设</a>。因此对<em>bigram</em>的联合概率计算公式化简为：
$$P(w_1^n)=P(w_1)P(w_2|w_1)P(w_3|w_1^2)...P(w_n|w_1^{n-1})$$
$$=\prod_{k=1}^nP(w_k|w_1^{k-1})$$
$$\approx \prod_{k=1}^{n}P(w_k|w_{k-1})$$</p>
<p>        对于$P(w_n|w_{n-1})$的计算我们引入极大似然估计。假设我们给定前置词$w_{i-1}$，要预测接下来单词$w_i$的概率。可以首先统计整个文档中$w_{i-1}w_i$组成的&quot;二元序列&quot;共同出现的次数$C(w_{i-1}w_i)$，再统计以$w_{i-1}$开头的全部&quot;二元序列&quot;的个数。则有：
$$P(w_n|w_{n-1})=\frac{C(w_{n-1}w_n)}{\sum_wC(w_{n-1}w)}$$</p>
<p>        事实上，分母中以$w_{i-1}$开头的全部&quot;二元序列&quot;的个数等价于$w_{i-1}$出现的次数。因此上式简化为:
$$P(w_n|w_{n-1})=\frac{C(w_{n-1}w_n)}{C(w_{n-1})}$$</p>
<h3>Example</h3>
<p>现有如下一段文字：
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">s</span>&gt;</span> I am Sam <span class="tag">&lt;/<span class="name">s</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">s</span>&gt;</span> Sam I am <span class="tag">&lt;/<span class="name">s</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">s</span>&gt;</span> I do <span class="tag">&lt;/<span class="name">s</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>        我们以<code>&lt;s&gt;</code>作为序列开始的标志，<code>&lt;/s&gt;</code>则作为结尾标志，它们也被当做文本中的一部分，现统计<em>bigram</em>如下：</p>
<p>$$\small P(I|&lt;s&gt;)=\frac{2}{3} \approx 0.67 \qquad P(I|Sam)=\frac{1}{2} = 0.5 \qquad P(am|I)=\frac{2}{3} \approx 0.67$$
$$\small P(Sam|&lt;s&gt;)=\frac{1}{3} \approx 0.33 \qquad P(do|I)=\frac{1}{3} \approx 0.33 \qquad P(&lt;/s&gt;|am)=\frac{1}{2} = 0.5$$
        此外我们还需要统计各个word出现的概率，以<em>Sam</em>为例，上文中<em>Sam</em>共出现了2次，不去重后总词数为14，因此：
$$\small P(Sam)=\frac{2}{14} \approx 0.14$$</p>
<p>由此我们可以计算出<em>bigram</em>下序列<code>Sam I am</code>的概率：
$$\small P(Sam \quad I \quad am) = P(Sam)P(I|Sam)P(am|Sam \quad I)$$
$$\small = P(Sam)P(I|Sam)P(am|I) \qquad (bigram假设)$$
$$\small = 0.14 \ast 0.5 \ast 0.67$$
$$\small = 0.0469$$
现在我们可以对任意一段序列计算其<em>bigram</em>下的概率了。</p>
<h3>推广到N-grams</h3>
<p>与<em>bigram</em>同理，广义上的N-grams是假设$history$仅与待预测的word前N个词相关:
$$P(w_n|w_{n-N+1}^{n-1}) = \frac{C(w_{n-N+1}^{n-1}w_n)}{C(w_{n-N+1}^{n-1})}$$
        接下来我们也能很快利用它计算基于<em>N-grams</em>的任意序列的概率。到此我们解决了上文中<strong>如何衡量序列A比序列B出现的概率高</strong>的问题。接下来我们将讨论如何预测下一个word。</p>
<h1>3 模型预测和评估</h1>
<h2>3.1 模型预测</h2>
<p>        我们给出对已知序列下一个词的预测算法：</p>
<ul>
<li>1.基于文本$t$，我们事先为其生成N-grams和(N-1)-grams。</li>
<li>2.给定值为{$w_1, w_2, ..., w_{n-1}$}的待预测序列$s$，目标是预测接下来的词$w_n$。我们仅取其近$N-1$个word，统计(N-1)-grams中序列$w_{n-N+1}^{n-1}$的频数$C_{w_{n-1}}$，并将N-grams中所有以$w_{n-N+1}^{n-1}$起始的序列中结尾词$w_n$加入候选集。</li>
<li>3.根据N-grams公式分别计算候选集中每一个word的条件概率$P(w_n|w_{n-N+1}^{n-1})$，从概率靠前的$topN$中随机挑选一个作为新word。</li>
<li>4.重复2-3步。</li>
</ul>
<p>        <a href="https://github.com/fancoo/N-Grams-novel" target="_blank" rel="external">N-Grams-novel</a>是我关于此算法的粗糙实现，它能够根据特定的训练文本和待预测序列，构建基于N-grams的&quot;自动小说生成器&quot;，效果如下：
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">/** 英文 */</div><div class="line">语料：Game of Thrones <span class="number">01</span> , 使用trigrams：</div><div class="line">待预测序列：</div><div class="line">There was <span class="keyword">an</span> edge <span class="keyword">to</span> this</div><div class="line">前<span class="number">50</span>词的生成结果：</div><div class="line"></div><div class="line">There was <span class="keyword">an</span> edge <span class="keyword">to</span> this . <span class="keyword">if</span> i have <span class="keyword">a</span> name <span class="keyword">to</span> <span class="keyword">make</span> the seas rise <span class="built_in">and</span> saw her <span class="keyword">first</span> man</div><div class="line">was more cunning than she had never known defeat . <span class="comment">" i am a sworn sword , and a third of those creatures ,</span></div><div class="line">whatever it takes . i have <span class="keyword">a</span> name ? oh</div><div class="line"></div><div class="line"></div><div class="line">/** 中文 */</div><div class="line">语料：《斗破苍穹》 , 使用trigrams：</div><div class="line">待预测序列：</div><div class="line">不愧是家族中种子级别的人物</div><div class="line">前<span class="number">200</span>词的生成结果：</div><div class="line"></div><div class="line">不愧是家族中种子级别的人物，她似乎便是从未见过萧炎白白吃亏过，即便如今已经晋入五星斗皇，</div><div class="line">那是一种凌驾这片天地之上的气息巨蟒逐渐消散，最后一卷被能量层所包裹的人影，萧炎也只得停下介绍，</div><div class="line">对着身旁那身姿欣长，脸色淡漠地举步走出，平静的声音，缓缓传出，青色斗气急速凝聚成火球之状，</div><div class="line">然后火球裂开一道缝隙，一颗足有半个拳头大小。</div><div class="line">在萧炎等人，也是逐渐地出现一丝晨辉时，萧炎却是一笑，道：“我知道你的一些事，只不过令得他感到意外，</div><div class="line">微微点了点头：“好了…”</div><div class="line">在萧炎等人为逃出一劫而松气时，那紧闭的眼眸，猛然睁开！</div><div class="line">在那众多惊骇目光中缓缓松开了手掌，切记，以你如今在加玛帝国，也顶多只能伤到二阶魔兽，想要击杀，</div><div class="line">却是必须跟老夫老老实实的交代，也是逐渐地变得黯淡</div></pre></td></tr></table></figure></p>
<h2>3.2 预测效果评估</h2>
<p>        可以看到，<strong>N-grams的预测风格完全取决于语料数据</strong>，因此我们要使用与预期结果文风相近的训练集。例如生成法律文档，就应该以法律文件作为训练集。注意到使用<em>trigrams</em>的效果已经可以得到语言相对连贯的句子了，那<em>bigrams</em>或<em>4-grams</em>效果会不会更好呢？我们需要一套指标衡量N取值为多少时预测效果更佳。</p>
<p>        对一个语言模型最好的评估方式是将其嵌入一款应用，然后看它提升了多少。这种端对端的评估称之为<strong>extrinsic evaluation</strong>，它是检验提升的唯一方法。基于此思想，我们的做法是：增加测试集。首先从训练集得到模型，然后评估它在测试集中的表现。评估准则是<strong>看哪一个模型对测试集词序列分配更高的概率——这表明它对预试集的预测更准确</strong>。例如测试集序列为：</p>
<p>                                                              <em>go back home</em></p>
<p>按我们之前对序列的计算公式，词序列<em>bigrams</em>概率为:
$$P_{bigram}(go　back　home)=P(go)P(back|go)P(home|back)$$
<em>trigrams</em>概率:
$$P_{trigram}(go　back　home)=P(go)P(back|go)P(home|go back)$$</p>
<p>很显然$P_{bigram} \ge P_{trigram}$，因此原来的计算方法评估并不合理。我们引入一种新的评估指标——Perplexity(混乱度)。</p>
<p>        <strong>Perplexity</strong>：一个测试样例的混乱度即为该测试样例的概率倒数，再对词数N做归一化。
$$PP(W)=P(w_1w_2...w_n)^{\frac{-1}{N}}$$
$$=\sqrt[N] {\frac{1}{P(w_1w_2...w_n)}}$$
$$=\sqrt[N] {\frac{1}{\prod_{k=1}^nP(X_k|X_1^{k-1})}}$$</p>
<p>        注意到条件概率乘积越高的序列，其perplexity越低。因此最小化PP等价于最大化测试集的概率。
我们从另一个角度考虑混乱度：即将其视为语言的<strong>带权分歧因子（weighted average branching factor）</strong>，<strong>分歧因子</strong>是指一个词后可能能跟的词的总个数。例如：在数字预测中，假设0-9这10个数字以概率1/10等可能地出现，那么
$$PP(W)=P(w_1w_2...w_n)^{\frac{-1}{N}}$$
$$={(\frac{1}{10})}^{N{(\frac{-1}{N})}}$$
$$={(\frac{1}{10})}^{-1}$$
$$=10$$
        但假如0在本文中出现比较频繁，可以证明PP将会降低，在这一点上PP和熵的特性很相似。</p>
<h1>4. 一些具体问题</h1>
<p>        我们已经介绍了如何计算序列的概率、如何生成新序列，以及模型好坏的评估手段，但实际应用中往往存在诸多问题。主要问题如下：</p>
<ul>
<li>注意到计算中我们的概率是乘积形式，如果这条&quot;链&quot;存在值为0怎么办？例如在计算$\small P(Sam \quad I \quad am)$中，若$\small P(am|I)$为0则整个句子概率为0，此特性在N取更大值时愈发明显，例如5-grams概率计算$\small P(home|Sam \quad and \quad I \quad went)$</li>
<li>如何处理测试集中的Unknown Words(生词)？</li>
</ul>
<h2>4.1 平滑处理</h2>
<p>解决上述乘积出现0的问题，方法是对数据做平滑处理。例如原本对<em>bigram</em>的计算：
$$P(w_n|w_{n-1})=\frac{C(w_{n-1}w_n)}{C(w_{n-1})}$$
使用Laplace平滑：
$$P_{Laplace}((w_n|w_{n-1})=\frac{C(w_{n-1}w_n)+1}{C(w_{n-1})+V}$$
其中V是去重后的总单词数。此项操作能使乘积&quot;链&quot;中不出现0。</p>
<p>        还有一种方法:倒退（Backoff）。例如要计算<em>trigram</em>条件概率$P(w_n|w_{n-2}w_{n-1})$，但我们并无$w_{n-2}w_{n-1}w_{n}$的词序列，这时候可以倒退回<em>bigram</em>并使用$P(w_n|w_{n-1})$，同理如果要计算<em>bigram</em>，但没有$P(w_n|w_{n-1})$，可以考虑计算$P(w_n)$，换句话说——在概率为0时只不断&quot;回退&quot;，直到概率不为0，但需要回退的概率计算结果做一定折扣。</p>
<p>        另一种是插值(Interpolation)：假如我们现在使用trigram，则<strong>每次预测</strong>（无论是否存在该trigram序列）均使用插值：
$$\hat{P}(w_n|w_{n-2}w_{n-1})=\lambda_1 P(w_n|w_{n-2}w_{n-1} )+ \lambda_2 P(w_n|w_{n-1}) + \lambda_3 P(w_n)$$
其中$\small \lambda$由新增一个训练集学习得出，并需要满足：
$$\sum_i \lambda_i = 1$$</p>
<h2>4.2 处理Unknown Words</h2>
<p>        最后我们简要谈谈Unknown Words，如果测试集中出现了训练集中没见过的词，我们就称之为的Unknown Words，这类词在训练集中概率为0，且前后N-gram词序列概率也为0。一个简单的处理办法是：对每个出现的Unknown Word替换为<code>&lt;UNK&gt;</code>，计算时以<code>&lt;UNK&gt;</code>的频率代替包含<code>&lt;UNK&gt;</code>序列的概率，更完善的解决方案可参考<a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf" target="_blank" rel="external">NLP Lunch Tutorial: Smoothing
</a>。</p>
<h1>5.文献及引用</h1>
<ul>
<li>[1]. <a href="https://github.com/evancchow/jazzml" target="_blank" rel="external">Github机器音乐项目:jazzml</a></li>
<li>[2]. <a href="https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf" target="_blank" rel="external">Speech and Language Processing Chap4:N-Grams</a></li>
<li>[3]. <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="external">Speech and Language Processing原书网站</a></li>
</ul>

 
                <!-- Meta -->
                <div class="post-meta">
                    <hr>
                    <br>
                    <div class="post-tags">
                        
                            

<a href="/tags/机器学习/">#机器学习</a> <a href="/tags/音乐/">#音乐</a> <a href="/tags/自然语言处理/">#自然语言处理</a>


                        
                    </div>
                    <div class="post-date">
                        2017 年 08 月 05 日
                    </div>
                </div>
            </div>

            <!-- Comments -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <!-- Disqus Comments -->


            </div>
        </div>
    </div>
</article>
</section>

    <!-- Scripts -->
    <!-- jQuery -->
<script src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script>
<!-- Bootstrap -->
<script src="//cdn.bootcss.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<script type="text/javascript">
	console.log('Hexo-theme-hollow designed by zchen9 🙋 © 2015-' + (new Date()).getFullYear());
</script>

    <!-- Google Analytics -->
    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-98699464-1', 'auto');
        ga('send', 'pageview');

    </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>

</html>